---
title: "R Notebook"
output: html_document
---

```{r}
library(dplyr)
library(readr)
library(rpart)
library(caret)
library(randomForest)
library(ggplot2)

set.seed(2021)

dataset_train <- read_csv("data/arbolado-memdoza-dataset.csv")
dataset_test <- read.csv("data/arbolado-mendoza-test.csv")

dataset_train$inclinacion_peligrosa <- as.factor(dataset_train$inclinacion_peligrosa)


#Dropeamos variables innecesarias para el modelo

drop <- names(dataset_train) %in% "ultima_modificacion"
dataset_train <- dataset_train[,!drop]
drop <- names(dataset_train) %in% "area_seccion"
dataset_train <- dataset_train[,!drop]
drop <- names(dataset_train) %in% "seccion"
dataset_train <- dataset_train[,!drop]
drop <- names(dataset_train) %in% "nombre_seccion"
dataset_train <- dataset_train[,!drop]


drop <- names(dataset_test) %in% "ultima_modificacion"
dataset_test <- dataset_test[,!drop]
drop <- names(dataset_test) %in% "area_seccion"
dataset_test <- dataset_test[,!drop]
drop <- names(dataset_test) %in% "seccion"
dataset_test <- dataset_test[,!drop]
drop <- names(dataset_test) %in% "nombre_seccion"
dataset_test <- dataset_test[,!drop]

#Crear nueva variable ratios
total_por_especie <- dataset_train %>% group_by(especie) %>% summarise(total = n(), total_peligroso = sum(inclinacion_peligrosa == 1))
ratios <- data.frame(especie = total_por_especie$especie, ratio = total_por_especie$total_peligroso/total_por_especie$total)

#Merge ratios
dataset_train <- merge(dataset_train, ratios, by = "especie")
dataset_train <- dataset_train[order(dataset_train$id),]

dataset_test <- merge(dataset_test, ratios, by = "especie")
dataset_test[order(dataset_test$id),]

#Separar en negativos y positivos ya que hay mucha mayor cantidad de negativos en el data_train
positivos <- dataset_train %>% filter(inclinacion_peligrosa == 1)
negativos <- dataset_train %>% filter(inclinacion_peligrosa == 0)

# Igualar el numero de positivos y negativos
negativospartition <- createDataPartition(negativos$especie,p=0.126 ,list=FALSE)
negativos <- negativos[ negativospartition,]
dataset_train <- rbind(negativos,positivos)



rf <- randomForest(inclinacion_peligrosa ~ ratio + lat + long + diametro_tronco,
                   data = dataset_train,
                   ntree = 600)

inclinacion_peligrosa <- predict(rf, newdata = dataset_test)


rf

inclinacion_peligrosa <- as.numeric(as.character(inclinacion_peligrosa))
resultados <- data.frame(id = dataset_test$id,inclinacion_peligrosa)


write.csv(resultados, "envio.csv")

# 1-) Se eliminaron las variables: “ultima_modificación”, “area_sección”, “seccion” y “nombre_seccion” de los sets.
#
# 2-) Se creó la variable “ratio” corresponde a el ratio de árboles con inclinación peligrosa por especie de árbol.
#
# 3-) No se normalizaron variables ya existentes, pero la variable creada ratio está normalizada ya que solo toma valores entre 0 y 1.
#
# B-) No se utilizó set de validación.
#
# C-) La puntuación obtenida en Kaggle fue de 0.71628
#
# D-) Se utilizo el algoritmo de Random Forest con 600 árboles de decisión para predecir la variable “inclinacion_peligrosa” utilizando como referencia los valores de las siguientes variables:
#
# Ratio: nos dice que tan propenso es a tener una inclinación peligrosa según su especie.
#
# Lat y Long: ya que si un árbol esta en condiciones de inclinación peligrosa, es probable que arboles cercanos estén en las mismas condiciones.
#
# Diámetro_tronco: para tener una medida de las dimensiones del árbol.
#
# Para el set de entrenamiento, se tomó el archivo de entrenamiento del desafío,
# se dividió en los árboles que tienen inclinación peligrosa y los que no,
# luego se tomó una partición de los que no tienen inclinación peligrosa que mantuviera una distribución de especies igual al set original de no peligrosos y que tuviera la misma cantidad de elementos que los arboles peligrosos,
# para que así se asemeje mas al set que se usa de test que tiene una distribución del 50/50.
```